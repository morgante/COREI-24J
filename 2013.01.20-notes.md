# Information Theory
Clause Shannon & Warren Weaver

Information and entropy defined in terms of probabilites. Low probability events have lots of information and conversely. Higher entropy circumstances require more information to describe.

Examine information gain: information necessary to determine (ex.) sickness knowing nothing else - information necessary of sick given that we know = 
I(sick)-I(sick|temp). Basically how much is gained from knowing each of these attributes.

## Information gain
Information = sum(i=1 < n characters)(P(i)*log(1/P(i))/log(2))